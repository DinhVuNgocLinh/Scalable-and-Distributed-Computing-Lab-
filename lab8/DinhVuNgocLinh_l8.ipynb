{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1RB3mdG2IIOUZhyJhCBO2q5rMwQli3iyP","timestamp":1716998392486}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1) How to convert the index of a PySpark DataFrame into a column?\n","\n","```python\n","# Input: Assuming df is your DataFrame\n","df = spark.createDataFrame([\n","(\"Alice\", 1),\n","(\"Bob\", 2),\n","(\"Charlie\", 3),\n","], [\"Name\", \"Value\"])\n","\n","df.show()\n","\n","+-------+-----+\n","| Name|Value|\n","+-------+-----+\n","| Alice| 1|\n","| Bob| 2|\n","|Charlie| 3|\n","+-------+-----+\n","\n","# Output:\n","+-------+-----+-----+\n","| Name|Value|index|\n","+-------+-----+-----+\n","| Alice| 1| 0|\n","| Bob| 2| 1|\n","|Charlie| 3| 2|\n","+-------+-----+-----+\n","```"],"metadata":{"id":"PqLMtw2_M1sW"}},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bch33dvZX63p","executionInfo":{"status":"ok","timestamp":1717003418398,"user_tz":-420,"elapsed":54833,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"d4a442ff-85d3-4651-edc1-1dde52df2cb0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=03f253645719b7bc043098db7ae7a0b571c52ce8acdc4ed5334e1aea65ed0f4e\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import monotonically_increasing_id\n","\n","# Initialize SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"IndexColumnExample\") \\\n","    .getOrCreate()\n","\n","# Assuming df is your DataFrame\n","df = spark.createDataFrame([\n","    (\"Alice\", 1),\n","    (\"Bob\", 2),\n","    (\"Charlie\", 3),\n","], [\"Name\", \"Value\"])\n","\n","# Add a new column with index\n","df = df.withColumn(\"index\", monotonically_increasing_id())\n","\n","# Show the DataFrame\n","df.show()\n","\n","# Stop SparkSession\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6LKtryVXUnj","executionInfo":{"status":"ok","timestamp":1717003852250,"user_tz":-420,"elapsed":22538,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"69b8331a-d289-4779-f5ab-e3dbdae0d23c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-----+----------+\n","|   Name|Value|     index|\n","+-------+-----+----------+\n","|  Alice|    1|         0|\n","|    Bob|    2|8589934592|\n","|Charlie|    3|8589934593|\n","+-------+-----+----------+\n","\n"]}]},{"cell_type":"markdown","source":["2) How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?\n","\n","Compute the minimum, 25th percentile, median, 75th, and maximum of column `Age`\n","\n","```python\n","# Create a sample DataFrame\n","data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n","df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n","```"],"metadata":{"id":"EnxZlhnhNRzQ"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Initialize SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"NumericColumnSummary\") \\\n","    .getOrCreate()\n","\n","# Create a sample DataFrame\n","data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n","df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n","\n","# Compute summary statistics\n","summary = df.select(\"Age\").summary(\"min\", \"25%\", \"50%\", \"75%\", \"max\")\n","\n","# Extract values from the summary DataFrame\n","min_age = summary.filter(col(\"summary\") == \"min\").select(\"Age\").collect()[0][0]\n","p25_age = summary.filter(col(\"summary\") == \"25%\").select(\"Age\").collect()[0][0]\n","median_age = summary.filter(col(\"summary\") == \"50%\").select(\"Age\").collect()[0][0]\n","p75_age = summary.filter(col(\"summary\") == \"75%\").select(\"Age\").collect()[0][0]\n","max_age = summary.filter(col(\"summary\") == \"max\").select(\"Age\").collect()[0][0]\n","\n","# Display the results\n","print(\"Minimum Age:\", min_age)\n","print(\"25th Percentile Age:\", p25_age)\n","print(\"Median Age:\", median_age)\n","print(\"75th Percentile Age:\", p75_age)\n","print(\"Maximum Age:\", max_age)\n","\n","# Stop SparkSession\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgLgW_b2avkn","executionInfo":{"status":"ok","timestamp":1717004098412,"user_tz":-420,"elapsed":10413,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"8b2ee790-1797-48c3-b165-f4d8d7fea69c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Minimum Age: 10\n","25th Percentile Age: 20\n","Median Age: 30\n","75th Percentile Age: 50\n","Maximum Age: 86\n"]}]},{"cell_type":"markdown","source":["3) Calculte the frequency counts of each unique value\n","\n","```python\n","from pyspark.sql import Row\n","\n","# Sample data\n","data = [\n","Row(name='John', job='Engineer'),\n","Row(name='John', job='Engineer'),\n","Row(name='Mary', job='Scientist'),\n","Row(name='Bob', job='Engineer'),\n","Row(name='Bob', job='Engineer'),\n","Row(name='Bob', job='Scientist'),\n","Row(name='Sam', job='Doctor'),\n","]\n","\n","# create DataFrame\n","df = spark.createDataFrame(data)\n","```"],"metadata":{"id":"MWhBlLpENtVs"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import Row\n","\n","# Initialize SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"ValueFrequencyCounts\") \\\n","    .getOrCreate()\n","\n","# Sample data\n","data = [\n","    Row(name='John', job='Engineer'),\n","    Row(name='John', job='Engineer'),\n","    Row(name='Mary', job='Scientist'),\n","    Row(name='Bob', job='Engineer'),\n","    Row(name='Bob', job='Engineer'),\n","    Row(name='Bob', job='Scientist'),\n","    Row(name='Sam', job='Doctor'),\n","]\n","\n","# create DataFrame\n","df = spark.createDataFrame(data)\n","\n","# Calculate frequency counts\n","freq_counts = df.groupBy(\"name\", \"job\").count()\n","\n","# Show the frequency counts\n","freq_counts.show()\n","\n","# Stop SparkSession\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ALdL-thb1s_","executionInfo":{"status":"ok","timestamp":1717004372942,"user_tz":-420,"elapsed":4304,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"bed475b1-27ff-4c76-c011-f0055d0dfa12"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+---------+-----+\n","|name|      job|count|\n","+----+---------+-----+\n","|Mary|Scientist|    1|\n","|John| Engineer|    2|\n","| Sam|   Doctor|    1|\n","| Bob| Engineer|    2|\n","| Bob|Scientist|    1|\n","+----+---------+-----+\n","\n"]}]},{"cell_type":"markdown","source":["4) How to keep only top 2 most frequent values as it is and replace everything else as `Other`?\n","\n","```python\n","from pyspark.sql import Row\n","\n","# Sample data\n","data = [\n","Row(name='John', job='Engineer'),\n","Row(name='John', job='Engineer'),\n","Row(name='Mary', job='Scientist'),\n","Row(name='Bob', job='Engineer'),\n","Row(name='Bob', job='Engineer'),\n","Row(name='Bob', job='Scientist'),\n","Row(name='Sam', job='Doctor'),\n","]\n","\n","# create DataFrame\n","df = spark.createDataFrame(data)\n","```"],"metadata":{"id":"0qoM05JdN-Xw"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import Row\n","from pyspark.sql.functions import col, desc, when\n","\n","# Initialize SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"Top2FrequentValues\") \\\n","    .getOrCreate()\n","\n","# Sample data\n","data = [\n","    Row(name='John', job='Engineer'),\n","    Row(name='John', job='Engineer'),\n","    Row(name='Mary', job='Scientist'),\n","    Row(name='Bob', job='Engineer'),\n","    Row(name='Bob', job='Engineer'),\n","    Row(name='Bob', job='Scientist'),\n","    Row(name='Sam', job='Doctor'),\n","]\n","\n","# Create DataFrame\n","df = spark.createDataFrame(data)\n","\n","# Calculate frequency counts\n","freq_counts = df.groupBy(\"job\").count()\n","\n","# Identify the top 2 most frequent values\n","top2 = freq_counts.orderBy(desc(\"count\")).limit(2).select(\"job\").rdd.flatMap(lambda x: x).collect()\n","\n","# Replace everything else with \"Other\"\n","df = df.withColumn(\"job\", when(col(\"job\").isin(top2), col(\"job\")).otherwise(\"Other\"))\n","\n","# Show the modified DataFrame\n","df.show()\n","\n","# Stop SparkSession\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_F-ZS_shdOPP","executionInfo":{"status":"ok","timestamp":1717004780799,"user_tz":-420,"elapsed":3793,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"be1de9fb-c9ba-4c3a-8e1b-93e0e7032208"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+---------+\n","|name|      job|\n","+----+---------+\n","|John| Engineer|\n","|John| Engineer|\n","|Mary|Scientist|\n","| Bob| Engineer|\n","| Bob| Engineer|\n","| Bob|Scientist|\n","| Sam|    Other|\n","+----+---------+\n","\n"]}]},{"cell_type":"markdown","source":["5) How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?\n","\n","```python\n","# suppose you have the following DataFrame\n","df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n","\n","# old column names\n","old_names = [\"col1\", \"col2\", \"col3\"]\n","\n","# new column names\n","new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n","```"],"metadata":{"id":"mtVhEU1qOLt8"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Initialize SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"ColumnRenameExample\") \\\n","    .getOrCreate()\n","\n","# Create DataFrame\n","df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n","\n","# Old column names\n","old_names = [\"col1\", \"col2\", \"col3\"]\n","\n","# New column names\n","new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n","\n","# Rename columns\n","df_renamed = df.select([col(old).alias(new) for old, new in zip(old_names, new_names)])\n","\n","# Show the renamed DataFrame\n","df_renamed.show()\n","\n","# Stop SparkSession\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdVKaqSYdoXb","executionInfo":{"status":"ok","timestamp":1717004877645,"user_tz":-420,"elapsed":2644,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"165f295d-6534-4ebd-9813-74746950e982"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+--------+--------+\n","|new_col1|new_col2|new_col3|\n","+--------+--------+--------+\n","|       1|       2|       3|\n","|       4|       5|       6|\n","+--------+--------+--------+\n","\n"]}]},{"cell_type":"markdown","source":["6) How to bin a numeric list to 10 groups of equal size?\n","\n","```python\n","from pyspark.sql.functions import rand\n","from pyspark.ml.feature import Bucketizer\n","\n","# Create a DataFrame with a single column \"values\" filled with random numbers\n","num_items = 100\n","df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n","```"],"metadata":{"id":"E4OfiEE9OiA-"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"UTmJ27kzMuyC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717005005362,"user_tz":-420,"elapsed":1889,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"5dee7dd9-025e-4807-8242-2308e7ef8e3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+---+\n","|              values|bin|\n","+--------------------+---+\n","|   0.619189370225301|6.0|\n","|  0.5096018842446481|5.0|\n","|  0.8325259388871524|8.0|\n","| 0.26322809041172357|2.0|\n","|  0.6702867696264135|6.0|\n","|  0.5173283545794627|5.0|\n","|  0.9991441647585968|9.0|\n","| 0.06993233728279169|0.0|\n","|  0.9696695610826327|9.0|\n","|  0.7959575617927873|7.0|\n","|  0.4484250584033179|4.0|\n","|  0.6793959570375868|6.0|\n","|  0.3724113862805264|3.0|\n","|   0.832609472539921|8.0|\n","|  0.7479557402720448|7.0|\n","|  0.7216183163402288|7.0|\n","|0.016051221049720343|0.0|\n","|  0.6307120027798567|6.0|\n","|    0.07537082371587|0.0|\n","|   0.838930558220017|8.0|\n","+--------------------+---+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import rand\n","from pyspark.ml.feature import Bucketizer\n","from pyspark.sql.functions import col\n","\n","# Initialize SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"NumericBinningExample\") \\\n","    .getOrCreate()\n","\n","# Create a DataFrame with a single column \"values\" filled with random numbers\n","num_items = 100\n","df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n","\n","# Define the number of bins\n","num_bins = 10\n","\n","# Calculate bin boundaries\n","min_value = df.agg({\"values\": \"min\"}).collect()[0][0]\n","max_value = df.agg({\"values\": \"max\"}).collect()[0][0]\n","bin_size = (max_value - min_value) / num_bins\n","bin_boundaries = [min_value + i * bin_size for i in range(num_bins + 1)]\n","\n","# Create Bucketizer transformer\n","bucketizer = Bucketizer(splits=bin_boundaries, inputCol=\"values\", outputCol=\"bin\")\n","\n","# Apply Bucketizer to the DataFrame\n","df_binned = bucketizer.transform(df)\n","\n","# Show the binned DataFrame\n","df_binned.select(\"values\", \"bin\").show()\n","\n","# Stop SparkSession\n","spark.stop()\n"]}]}