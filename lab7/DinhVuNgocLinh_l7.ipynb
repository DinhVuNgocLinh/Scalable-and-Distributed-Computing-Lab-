{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1B1sXHOSQVV4CcVnLvGvi0KD_n7OOCN5A","timestamp":1716998175494}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["I. Theoretical session:\n","1. Could you list out some limitation of MapReduce?\n","2. Provide a high level comparison of Apache Hadoop and Apache Spark.\n","3. What are the advantages of Apache Spark?\n","4. Provide a comparison of RDD and DataFrame in Spark.  "],"metadata":{"id":"V-S6ggQMmxvx"}},{"cell_type":"markdown","source":["1. Limitations of MapReduce:MapReduce programs is time-consuming and complex. It is designed for batch processing and is not suitable for real-time data processing. Moreover, Cumbersome and less user-friendly. Cannot handle quick, interactive queries. Inefficient for iterative tasks due to disk I/O. Significant overhead due to data shuffling and sorting.\n","2.\n","**Apache Hadoop**:\n","- Open-source software for managing big data sets.\n","- Highly scalable and cost-effective.\n","- Supports advanced analytics.\n","- Consists of HDFS, YARN, MapReduce, and Hadoop Common.\n","\n","**Apache Spark**:\n","- Data processing engine for big data.\n","- Faster than Hadoop, uses RAM for data processing.\n","- Ideal for machine learning and large-scale data.\n","- More advanced, uses AI/ML in data processing.\n","- Can be 100x faster than Hadoop for smaller workloads.\n","\n","**Similarities**:\n","- Both are distributed systems for processing data at scale.\n","- Can recover from failure.\n","- Composed of several software modules.\n","\n","3. Advantages of Apache Spark\n","* Speed:\n","Spark processes data in memory, which significantly boosts performance, especially for iterative algorithms.\n","* Ease of Use:\n","Provides high-level APIs in Java, Scala, Python, and R, along with an interactive shell.\n","* Unified Engine:\n","Supports a wide range of workloads, including batch processing, streaming, interactive queries, and machine learning.\n","* Advanced Analytics:\n","Includes advanced libraries such as MLlib for machine learning, GraphX for graph processing, and Spark SQL for structured data processing.\n","* Real-time Stream Processing:\n","Spark Streaming allows for real-time data processing with low latency.\n","* Fault Tolerance:\n","Uses Resilient Distributed Datasets (RDDs) to recover lost data and recompute data efficiently.\n","4.\n","**RDD**:\n","- Fundamental data structure in Spark.\n","- Offers low-level transformations and actions.\n","- No performance optimization.\n","- Useful for unstructured data.\n","- Good for complex tasks requiring fine-tuned transformations.\n","\n","**DataFrame**:\n","- Distributed collection of data organized into named columns.\n","- Higher level of abstraction than RDDs.\n","- More expressive and efficient due to the Catalyst Optimizer.\n","- Allows imposing a structure onto a distributed collection of data.\n","- Supports a wide range of operations and transformations."],"metadata":{"id":"FAS9lb9FGaSz"}},{"cell_type":"markdown","source":[],"metadata":{"id":"J8pQB-VMGXEk"}},{"cell_type":"markdown","source":["II. You are given a file `appl_stock.csv`, please carry out the following tasks:\n","\n","1. Read this file by PySpark. Print out the schema.\n","2. Create new columns of combining the High, Low, Close and Adj Close as follow `[High, Low, Close, Adj Close]`.\n","3. Create a new column which computes the average price of High and Low prices.\n","4. Create a new column which computes the amount of money based on the formula `Volume * Adj Close`.\n","3. Using `groupby` and `year()` function to compute the average closing price per year.\n"],"metadata":{"id":"S9_563ulpsh9"}},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVO9TsSlOHrl","executionInfo":{"status":"ok","timestamp":1717000828827,"user_tz":-420,"elapsed":59331,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"02cf37e7-6364-4d74-8a54-207690684ae6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=3b30fa375068023aa184639fc0746aa89db81c82556d0df66c985ab2fee27ccb\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, year, avg, expr\n","\n","# Initialize a Spark session\n","spark = SparkSession.builder.appName(\"StockAnalysis\").getOrCreate()\n","\n","drive.mount('/content/drive')\n","file_path = '/content/drive/My Drive/Colab Notebooks/Scalable and Distributed Computing/Lab/lab7/appl_stock.csv'\n","df = spark.read.csv(file_path, header=True, inferSchema=True)\n","\n","# Print the schema\n","df.printSchema()\n","\n","# Combine High, Low, Close, and Adj Close into a new column\n","df = df.withColumn(\"Combined_High_Low_Close_AdjClose\", expr(\"array(High, Low, Close, `Adj Close`)\"))\n","\n","# Create a new column for the average price of High and Low prices\n","df = df.withColumn(\"Average_High_Low\", (col(\"High\") + col(\"Low\")) / 2)\n","\n","# Create a new column for the amount of money using Volume * Adj Close\n","df = df.withColumn(\"Amount_of_Money\", col(\"Volume\") * col(\"Adj Close\"))\n","\n","# Compute the average closing price per year\n","df_with_year = df.withColumn(\"Year\", year(col(\"Date\")))\n","avg_close_per_year = df_with_year.groupBy(\"Year\").agg(avg(\"Close\").alias(\"Average_Closing_Price\"))\n","\n","# Show the results\n","avg_close_per_year.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQ_t2ndwMe1E","executionInfo":{"status":"ok","timestamp":1717001017588,"user_tz":-420,"elapsed":31910,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"086bfe7d-1131-48e4-d61b-d2a25292b310"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","root\n"," |-- Date: date (nullable = true)\n"," |-- Open: double (nullable = true)\n"," |-- High: double (nullable = true)\n"," |-- Low: double (nullable = true)\n"," |-- Close: double (nullable = true)\n"," |-- Volume: integer (nullable = true)\n"," |-- Adj Close: double (nullable = true)\n","\n","+----+---------------------+\n","|Year|Average_Closing_Price|\n","+----+---------------------+\n","|2015|   120.03999980555547|\n","|2013|    472.6348802857143|\n","|2014|    295.4023416507935|\n","|2012|    576.0497195640002|\n","|2016|   104.60400786904763|\n","|2010|    259.8424600000002|\n","|2011|   364.00432532142867|\n","+----+---------------------+\n","\n"]}]},{"cell_type":"markdown","source":["III. You are given a data `customer_churn.csv`, which describes the churn status in clients of a marletting agency. As a data scientist, you are required to create a machine learning model **in Spark** that will help predict which customers will churn (stop buying their service). A short description of the data is as follow:\n","```\n","Name : Name of the latest contact at Company\n","Age: Customer Age\n","Total_Purchase: Total Ads Purchased\n","Account_Manager: Binary 0=No manager, 1= Account manager assigned\n","Years: Totaly Years as a customer\n","Num_sites: Number of websites that use the service.\n","Onboard_date: Date that the name of the latest contact was onboarded\n","Location: Client HQ Address\n","Company: Name of Client Company\n","```\n","\n","1. Read, print the schema and check out the data to set the first sight of the data.\n","2. Format the data according to `VectorAssembler`, which is supported in MLlib of PySpark.\n","3. Split the data into train/test data, and then fit train data to the logistic regression model.\n","4. Evaluate the results and compute the AUC."],"metadata":{"id":"brQ8gRaUshXy"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.ml import Pipeline\n","\n","# Initialize a Spark session\n","spark = SparkSession.builder.appName(\"CustomerChurnPrediction\").getOrCreate()\n","\n","# Load the CSV data into a Spark DataFrame\n","file_path = '/content/drive/My Drive/Colab Notebooks/Scalable and Distributed Computing/Lab/lab7/customer_churn.csv'\n","df = spark.read.csv(file_path, header=True, inferSchema=True)\n","\n","# Print the schema of the DataFrame\n","df.printSchema()\n","\n","# Show the first few rows of the DataFrame\n","df.show(10)\n","\n","# Select the relevant features and label for the model\n","feature_cols = [\"Age\", \"Total_Purchase\", \"Account_Manager\", \"Years\", \"Num_Sites\"]\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","# Initialize the Logistic Regression model\n","lr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\")\n","\n","# Create a pipeline with the assembler and logistic regression\n","pipeline = Pipeline(stages=[assembler, lr])\n","\n","# Prepare the data for training\n","data = df.select(feature_cols + [\"Churn\"])\n","\n","# Split the data into train and test sets\n","train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n","\n","# Fit the model on the training data\n","model = pipeline.fit(train_data)\n","\n","# Make predictions on the test data\n","predictions = model.transform(test_data)\n","\n","# Evaluate the model using AUC\n","evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n","auc = evaluator.evaluate(predictions)\n","\n","print(f\"AUC: {auc}\")\n","\n","# Stop the Spark session\n","spark.stop()\n"],"metadata":{"id":"fN4Zb88PtzCL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717002616065,"user_tz":-420,"elapsed":7326,"user":{"displayName":"Linh ẩn tên","userId":"08735734188666953550"}},"outputId":"dc0a7e64-d9d8-45cd-f8e2-9a99a5cf520e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Names: string (nullable = true)\n"," |-- Age: double (nullable = true)\n"," |-- Total_Purchase: double (nullable = true)\n"," |-- Account_Manager: integer (nullable = true)\n"," |-- Years: double (nullable = true)\n"," |-- Num_Sites: double (nullable = true)\n"," |-- Onboard_date: timestamp (nullable = true)\n"," |-- Location: string (nullable = true)\n"," |-- Company: string (nullable = true)\n"," |-- Churn: integer (nullable = true)\n","\n","+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n","|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n","+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n","|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n","|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n","|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n","|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|\n","|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|\n","|Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|2009-03-03 23:13:37|6187 Olson Mounta...|        Kelly-Warren|    1|\n","|     Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|2016-12-05 03:35:43|4846 Savannah Roa...|   Reynolds-Sheppard|    1|\n","|   Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|2006-03-09 14:50:20|25271 Roy Express...|          Singh-Cole|    1|\n","|     Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|2011-09-29 05:47:23|3725 Caroline Str...|           Lopez PLC|    1|\n","|  Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|2006-03-28 15:42:45|363 Sandra Lodge ...|       Reed-Martinez|    1|\n","+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n","only showing top 10 rows\n","\n","AUC: 0.9232723577235776\n"]}]}]}