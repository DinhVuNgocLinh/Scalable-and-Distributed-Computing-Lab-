{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1lIiWqUZOS9if7-GqQD-VQriv23CV92wJ","timestamp":1713777704500}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Exercise I\n","\n","The input is a textual csv file containing the daily value of PM10 for a set of sensors, and in each line of the files has the following format:\n","```sensorId,date,PM10 value (Î¼g/m3)\\n```\n","\n","Here is the example of data:\n","```\n","s1,2016-01-01,20.5\n","s2,2016-01-01,30.1\n","s1,2016-01-02,60.2\n","s2,2016-01-02,20.4\n","s1,2016-01-03,55.5\n","s2,2016-01-03,52.5\n","```\n","\n","You're required to use pyspark to load the file, filter the values and use map/reduce code idea to give the output. The output is a line for each sensor on the standard output.\n","Each line contains a `sensorId` and the list of `dates` with a PM10 values greater than 50 for that sensor. The example output:\n","```\n","(s1, [2016-01-02, 2016-01-03])\n","(s2, [2016-01-03])\n","```\n","\n"],"metadata":{"id":"8H58RnChZq0b"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, collect_list\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.appName(\"PM10Filter\").getOrCreate()\n","\n","# Load the CSV file\n","df = spark.read.csv(\"sensors.csv\", header=False)\n","\n","# Rename columns\n","df = df.withColumnRenamed(\"_c0\", \"sensorId\") \\\n","       .withColumnRenamed(\"_c1\", \"date\") \\\n","       .withColumnRenamed(\"_c2\", \"PM10\")\n","\n","# Filter values greater than 50\n","filtered_df = df.filter(col(\"PM10\") > 50)\n","\n","# Group by sensorId and collect dates\n","result_df = filtered_df.groupBy(\"sensorId\").agg(collect_list(\"date\").alias(\"dates\"))\n","\n","# Show the result\n","result_df.show(truncate=False)\n","\n","# Stop the SparkSession\n","spark.stop()\n"],"metadata":{"id":"fBhuqQxI17wf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise II\n","\n","Using the same data of the Exercise I, you're required to get the output: sensors ordered by the number of critical days. Each line of the output contains the number of days with a PM10 values greater than 50 for a sensor `s` and the `sensorId` of sensor `s`.\n","\n","The example of the output:\n","```\n","2, s1\n","1, s2\n","```\n","\n"],"metadata":{"id":"Jgu0vQKVbqDf"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, size\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.appName(\"CriticalDays\").getOrCreate()\n","\n","# Load the CSV file\n","df = spark.read.csv(\"sensors.csv\", header=False)\n","\n","# Rename columns\n","df = df.withColumnRenamed(\"_c0\", \"sensorId\") \\\n","       .withColumnRenamed(\"_c1\", \"date\") \\\n","       .withColumnRenamed(\"_c2\", \"PM10\")\n","\n","# Filter values greater than 50\n","filtered_df = df.filter(col(\"PM10\") > 50)\n","\n","# Group by sensorId and count number of critical days\n","result_df = filtered_df.groupBy(\"sensorId\").agg(size(collect_list(\"date\")).alias(\"num_critical_days\"))\n","\n","# Order by number of critical days\n","result_df = result_df.orderBy(\"num_critical_days\", ascending=False)\n","\n","# Show the result\n","result_df.show(truncate=False)\n","\n","# Stop the SparkSession\n","spark.stop()\n"],"metadata":{"id":"CWhmCn2UYuAs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise III\n","\n","In this exercise, you're given an input: A CSV file containing a list of profiles\n","\n","- Header: `name,age,gender`\n","- Each line of the file contains the information about one user\n","\n","The example of input data\n","```\n","name,surname,age\n","Paolo,Garza,42\n","Luca,Boccia,41\n","Maura,Bianchi,16\n","```\n","\n","You're required to use pyspark to load and analyze the data to achieve the output: A CSV file containing one line for each profile. The original age attribute is substituted with a new attributed called rangeage of type String.\n","```\n","rangeage = \"[\" + (age/10)*10 + \"-\" + (age/10)*10+9 + \"]\"\n","```\n","\n","\n","\n"],"metadata":{"id":"ADGjGNWKePfN"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, concat, lit, floor\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.appName(\"ageRange\").getOrCreate()\n","\n","# Load the CSV file\n","df = spark.read.csv(\"profiles.csv\", header=True)\n","\n","# Define the transformation\n","df = df.withColumn(\"rangeage\",\n","                   concat(\n","                       lit('['),\n","                       (floor(df[\"age\"]/10)*10).cast(\"string\"),\n","                       lit('-'),\n","                       ((floor(df[\"age\"]/10)*10)+9).cast(\"string\"),\n","                       lit(']')\n","                   )\n","                  )\n","\n","# Drop the original 'age' column\n","df = df.drop('age')\n","\n","# Write the result back to a new CSV file\n","df.write.csv(\"profiles_with_rangeage.csv\", header=True)\n","\n","# Stop the SparkSession\n","spark.stop()\n"],"metadata":{"id":"igKez2B6aPe0"},"execution_count":null,"outputs":[]}]}